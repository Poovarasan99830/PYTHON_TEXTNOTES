

# ---_______________________________________________________________________

## ğŸ”¹ 2ï¸âƒ£ K-Nearest Neighbours (KNN) â€” â€œKooda irukura pasanga pola decide panra modelâ€



### ğŸ§© 1ï¸âƒ£ **Real-World Intuition â€“ Basic Thought**

Imagine you are a teacher in a new school.
A new student **Arun** joins.
You donâ€™t know his character â€” whether heâ€™s **studious** or **naughty**.

What do you do?

â¡ï¸ You look at **his friends**.
If most of his close friends are **studious**, you assume *Arun also studious.*
If most are **naughty**, then *Arun also naughty.*

ğŸ‘‰ Thatâ€™s exactly **KNNâ€™s idea**.

> "Oru new data vandha, adha kooda irukura pasanga (nearest points) paathu decide pannum."



# ---_______________________________________________________________________




### âš™ï¸ 2ï¸âƒ£ **Mechanism â€“ How KNN Works (Step by Step)**

#### Step 1: Store Data (Training Phase)


KNN doesnâ€™t â€œlearnâ€ any formula or pattern.
It simply **stores all training data points** in memory.

Thatâ€™s why itâ€™s called a **lazy learner** â€”
it does the real work only **when a new input** comes.




# ---_______________________________________________________________________



#### Step 2: New Input Arrives (Prediction Phase)

Suppose a new point ( x' ) comes in.

Now, KNN will:

1. Calculate the **distance** between ( x' ) and every training point ( x_i ).

   * Commonly, use **Euclidean Distance**:
     [
     d(x', x_i) = \sqrt{\sum_{j=1}^n (x'*j - x*{ij})^2}
     ]
   * Simple version: â€œMeasure how far the new point is from each existing one.â€

2. Sort all training points by **distance** (smallest first).

3. Choose **K nearest neighbours** (the top K smallest distances).


# --- _______________________________________________________________________

#### Step 3: Voting or Averaging

Now:

* For **classification** â†’ pick the **majority class** among K.
* For **regression** â†’ take the **average value** of K.

Example:
If K=5 and among 5 nearest, 3 are ğŸ (apple) and 2 are ğŸŠ (orange)
â†’ classify as **apple**.

# --- _______________________________________________________________________

#### Step 4: Output

Thatâ€™s your predicted class/value for the new point.

---

### ğŸ§® 3ï¸âƒ£ **The Mathematical Core (in Simple Words)**

For a new input ( x' ):

[
\hat{y} = \text{Majority Vote}{ y_i : x_i \in N_K(x') }
]

Where:

* ( N_K(x') ) = K nearest training points to ( x' )
* ( y_i ) = their corresponding labels

# ---_______________________________________________________________________

### ğŸ“ˆ 4ï¸âƒ£ **Geometry â€“ Whatâ€™s Happening Visually**

Imagine 
red âŒ 
blue ğŸ”µ 

points on a 2D graph.

KNN divides the space into **regions**.
Each region belongs to the **majority color** of nearby points.

â¡ï¸ So, if you drop a new dot into that space,
it will **inherit the color of its closest neighbors.**

Thatâ€™s why KNN often forms **curvy, non-linear boundaries** â€”
it simply follows the *shape* of the data.



# ---_______________________________________________________________________



### ğŸ§  5ï¸âƒ£ **Key Intuitions (From First Principles)**

| Concept            | Simple Tanglish Explanation                                         |
| ------------------ | ------------------------------------------------------------------- |
| **Learning?**      | Illa, data save pannum (lazy learner).                              |
| **Decision?**      | Neighbours la majority yaar-nu paathu.                              |
| **Formula?**       | Just distance calculation (no weights, no training).                |
| **Complexity?**    | Training easy, testing time heavy (because distance to all points). |
| **Scaling issue?** | Yes â€” large data = more computation.                                |



# ---_______________________________________________________________________

### âš–ï¸ 6ï¸âƒ£ **When to Use / Avoid**

| Use When                               | Avoid When                          |
| -------------------------------------- | ----------------------------------- |
| Small dataset                          | Very large dataset (slow)           |
| Data is numeric (distance makes sense) | Features have very different scales |
| You expect **local patterns**          | You need explainable formulas       |

# ---_______________________________________________________________________

### ğŸ§© 7ï¸âƒ£ **Example â€” Fruit Classification**

| Fruit     | Weight (g) | Sweetness | Type   |
| --------- | ---------- | --------- | ------ |
| ğŸ Apple  | 200        | 8         | Apple  |
| ğŸŠ Orange | 180        | 4         | Orange |
| ğŸ Apple  | 220        | 9         | Apple  |
| ğŸŠ Orange | 160        | 3         | Orange |

Now, new fruit: Weight=210, Sweetness=7

Compute distance to all points â†’ pick **K=3** nearest â†’
if 2 are Apple, 1 Orange â†’ classify as ğŸ Apple.

---

### ğŸ§­ 8ï¸âƒ£ **Essence â€” One-Line Formula in Tanglish**

> â€œKNN = oru new data vandha, adha kooda irukura pasanga paathu decide pannum.â€
> â€œNo brain (training), only heart (neighbours).â€ â¤ï¸

# ---_______________________________________________________________________

### ğŸ§® 9ï¸âƒ£ **Real-World Example**

* Recommender Systems:
  â€œIntha user ku pudicha movies maari vera yaar yaar pudichiruka?â€
  â†’ Similar usersâ€™ favourites recommend pannum.

* Image Classification:
  Pixel features la similar image find pannum.

# ---_______________________________________________________________________

### ğŸ§± 10ï¸âƒ£ **KNN From First Principles (Summary)**

| Step | Concept    | What Happens                        |
| ---- | ---------- | ----------------------------------- |
| 1    | Store Data | No model, just keep all data points |
| 2    | New Input  | Compute distances                   |
| 3    | Sort       | Find nearest K                      |
| 4    | Vote       | Choose majority class               |
| 5    | Output     | Return predicted class/value        |



# ---_______________________________________________________________________




