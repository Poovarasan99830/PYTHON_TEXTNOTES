

_____________________________________________________________________________________

What is LLM ?
_____________________________________________________________________________________



â€œLLM learns language structure using transformers,
 but frameworks like LangChain give it logic, memory, and real-world control.â€ âš™ï¸


 _____________________________________________________________________________________

What is LongChain?
_____________________________________________________________________________________




# LangChain = the bridge between LLMs and real-world applications.


CAB
C=connecting external db,
  connect your llm(like open AI)to outside data sources(google search,SQl,pdf,api)

A=Add Memory(so its remembers what happened earlier)

B=Build chain step by step work flow(get data -->analyze--->return answer)

LLM brain, LangChain brain control system, RAG data memory, FastAPI frontend face





| Example          | Role                                                                                                  |
| ---------------- | ----------------------------------------------------------------------------------------------------- |
| ğŸ—£ï¸ **LLM**      | Like ChatGPT itself â€” understands and replies in language.                                            |
| ğŸ§° **LangChain** | Like a *project manager* who decides when to ask ChatGPT, when to call APIs, when to store data, etc. |




_____________________________________________________________________________________
 Draw data flow (user â†’ LLM â†’ response)?
_____________________________________________________________________________________



â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        AI Agent              â”‚
â”‚  (LangChain Framework)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              â”‚               â”‚
â–¼              â–¼               â–¼
LLM            Tools           Memory
(GPT/Claude)   (APIs, DBs)     (Conversation Context)



â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                USER

ğŸ§‘â€ğŸ’»  User asks:  "Show me latest AI research papers"
â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                LLM (GPT / Claude)

ğŸ§   Understands question  
ğŸ¤”  Decides: "I need to use the Arxiv Tool to find papers"
â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              LANGCHAIN TOOL

ğŸ”§ Tool Name:  ArxivQueryRun  
ğŸ§© Function:   Uses ArxivAPIWrapper  
ğŸ’¬ Description: "Fetch papers from the arXiv database"

ğŸ‘‰ The tool calls a **Python function or API** defined inside LangChain
â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              EXTERNAL API (REAL DATA)

ğŸŒ  API:  https://export.arxiv.org/api/query  
ğŸ“š  Data Source:  Research papers repository  
â¬…ï¸  Returns: Titles, authors, abstracts, links
â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              LANGCHAIN TOOL

ğŸ§© Receives JSON/XML data from arXiv  
ğŸ“¦ Parses and sends clean text back to LLM
â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                LLM (GPT / Claude)

ğŸ§  Reads abstracts  
ğŸª„ Summarizes papers  
ğŸ—£ï¸ Prepares a natural-language answer
â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    USER

ğŸ’¬ "Here are top 3 recent AI papers from arXiv:  
1ï¸âƒ£ â€¦  
2ï¸âƒ£ â€¦  
3ï¸âƒ£ â€¦"
streamlit run app.py



â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ§­ CASE A: MEMORY MISS (First Time)
User Query
   â†“
Memory âŒ Not Found
   â†“
LLM â†’ Uses Tool â†’ API Call â†’ Summarize
   â†“
Store Result â†’ Return Response



â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ§  CASE B: MEMORY HIT (Later Query)
User Query
   â†“
Memory âœ… Found
   â†“
LLM â†’ Uses Context â†’ Generate Response
   â†“
Return Response (No Tool Needed)





# _____________________________________
     ***####  Memory ####*****
# _____________________________________





User Query
   â”‚
   â–¼
Check Memory
   â”œâ”€â”€ âœ… Found â†’ Send context â†’ LLM â†’ Respond
   â””â”€â”€ âŒ Not Found
           â†“
         LLM Reasoning
           â†“
        Decide Tool Call
           â†“
        TMDb API â†’ Data
           â†“
        LLM Summarize + Store in Memory
           â†“
        Return Response


Memory check happens before LLM call.

LLM never calls tools if memory already satisfies the question.

Short-term memory = session context.

Long-term (Vector DB) = cross-session recall (knowledge retention).


1ï¸âƒ£ when memory already has the answer, and
2ï¸âƒ£ when no memory is found â†’ LLM decides to use a Tool (like TMDb API).


ğŸ§  Memory is always checked first.
If it fails â†’ the LLM decides which tool (API, DB, etc.) to use â†’ then the result is stored back into memory for next time.





| Layer               | Tool / Framework        | Description                     |
| ------------------- | ----------------------- | ------------------------------- |
| **Frontend**        | Streamlit / React       | User Interface                  |streamlit run app.py
| **API Layer**       | FastAPI / Flask         | Communication + Logic           |
| **Orchestration**   | LangChain               | Manages flow & prompt logic     |
| **LLM**             | GPT-5 / Claude / Gemini | Generates summaries             |
| **External Source** | arXiv API               | Paper data source               |
| **Database**        | SQLite / PostgreSQL     | Stores queries, papers, results |



| Layer                   | What Happens                                |
| ----------------------- | ------------------------------------------- |
| **LLM (GPT)**           | Thinks, reasons, and writes text            |
| **LangChain Engine**    | Controls the logic (who calls what)         |
| **Tool (Custom)**       | Sends real HTTP request to TMDb             |
| **External API (TMDb)** | Returns JSON data                           |
| **Response Path**       | API â†’ Tool â†’ LangChain â†’ LLM â†’ Backend â†’ UI |

streamlit run app.py



_____________________________________________________________________________________
LANGCHAIN â€” CONCEPTS SUMMARY ?
_____________________________________________________________________________________



â€œLangChain is not an AI model â€” itâ€™s the framework that helps manage how AI models (LLMs) interact with tools, memory, and external data in a structured, reusable way.


| Concept       | Role              | Analogy           |
| ------------- | ----------------- | ----------------- |
| LLM           | Brain             | Thinker           |
| Prompt        | Instruction       | Question format   |
| Chain         | Workflow          | Assembly line     |
| Memory        | Context storage   | Short-term memory |
| Agent         | Decision maker    | Manager           |
| Tool          | Helper function   | Worker            |
| Retriever     | Knowledge fetcher | Google search     |
| Output Parser | Formatter         | Data cleaner      |
| Callback      | Tracker           | Logger            |
| LangSmith     | Debugger          | QA Tester         |



https://chatgpt.com/share/69123a1b-8da8-800a-88af-51cbc57e962c

_____________________________________________________________________________________
Build LangChain + OpenAI mini chatbot ?
_____________________________________________________________________________________



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                      EduBot Architecture
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[User Browser / Streamlit UI]
  - sends user query (POST / via streamlit input)
  - receives rendered response

        â”‚
        â–¼

[Frontend (Streamlit app)]
  - collects user_input
  - displays chat history
  - calls local chat function `chat(user_input)` (no public API required)
  - reads environment variables at startup

        â”‚
        â–¼

[Application Logic Layer]
  - chatbot.bot_core.create_edubot() returns chat() function
  - manages in-memory or persistent history (ChatMessageHistory)
  - constructs message list (system prompt + history + user message)
  - handles LLM call + exceptions + fallback logic

        â”‚
        â–¼

[LLM Provider Adapter (langchain_groq)]
  - ChatGroq model invocation (model ID exactly matching provider)
  - uses GROQ_API_KEY from environment
  - possible responses: success | model_deprecated | rate-limit | quota error

        â”‚
        â–¼
[External LLM Service: Groq]
  - runs Llama models (e.g., llama-3.1-8b-instant)
  - returns text response or error codes (handle them gracefully)

        â”‚
        â–¼

[Optional: Persistence]
  - Save conversation to SQLite / Supabase / Firebase
  - Use for long-term memory, analytics, or per-user history

        â”‚
        â–¼

[Monitoring & Logging]
  - Console logs + file logs
  - Optionally use LangSmith / Sentry for observability






  
LangChain = toolkit
Groq API = communication channel
Model (llama-3.1) = actual brain
LLM = general term for that kind of AI brain
EduBot = your final assembled app that uses all of them.




| Principle                | Developer Reality                           | Analogy                 |
| ------------------------ | ------------------------------------------- | ----------------------- |
| **Architecture**         | Transformer design defines how model thinks | Brain wiring            |
| **Parameters (8B, 70B)** | Model capacity / intelligence               | Number of neurons       |
| **Tokenizer**            | Converts words â†” numbers                    | Dictionary of syllables |
| **Context Window**       | How much text model â€œremembersâ€             | Short-term memory       |
| **Version / Flavor**     | Indicates speed / capability trade-offs     | Textbook editions       |
| **Hosted Model**         | API-managed LLM                             | Cloud classroom         |
| **API ID**               | Modelâ€™s code name                           | Product SKU             |







| Step    | Layer                  | Description                           | Example                         |
| ------- | ---------------------- | ------------------------------------- | ------------------------------- |
| **1ï¸âƒ£** | Frontend               | User inputs query in Streamlit        | `"Explain Python decorators"`   |
| **2ï¸âƒ£** | Prompt Builder         | LangChain builds conversation context | System + history + user         |
| **3ï¸âƒ£** | LLM Wrapper (ChatGroq) | Converts prompt â†’ API request         | model, temp, key                |
| **4ï¸âƒ£** | Groq API               | Executes model inference              | `llama-3.1-8b-instant`          |
| **5ï¸âƒ£** | Response Handling      | LangChain wraps + returns reply       | `AIMessage(content=...)`        |
| **6ï¸âƒ£** | Frontend Display       | Streamlit shows output                | EduBotâ€™s answer                 |
| **7ï¸âƒ£** | Memory Update          | Adds both messages to history         | Enables conversation continuity |



# __________________________________________________________________________________
*          [ ] Day 8: Dissect 1 new library (ChromaDB) â†’ folder + API flow
# __________________________________________________________________________________


ChromaDB stores **vectors**, **documents**, and **metadata**, enabling fast information retrieval based on meaning.



my_rag_app/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ chroma/            # ChromaDB persistent storage
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py          # Add documents + embed
â”‚   â”œâ”€â”€ query.py           # Query the DB
â”‚   â””â”€â”€ utils.py           # Helpers
â”‚
â””â”€â”€ requirements.txt





## **5. Core API Flow (Explained)**
          Raw Text â†’ Embedding â†’ Stored in ChromaDB â†’ Semantic Search â†’ Relevant Output





## **10. ChromaDB + LangChain Flow**
            Documents â†’ Embeddings â†’ Chroma Vector Store â†’ LangChain Retriever â†’ LLM




## **15. Architecture Diagram (Text-Based)**

```
              +------------------------+
              |      Your Dataset      |
              |  (PDF, TXT, HTML, etc) |
              +-----------+------------+
                          |
                          v
                 +--------+--------+
                 |  Text Splitter   |
                 |  (Chunking)      |
                 +--------+--------+
                          |
                          v
             +------------+-------------+
             |   Embedding Model        |
             | (OpenAI / HF / Others)   |
             +------------+-------------+
                          |
                          v
          +---------------+-----------------+
          |              ChromaDB           |
          |  - Store vectors                |
          |  - Store documents              |
          |  - Store metadata               |
          |  - Semantic search              |
          +---------------+-----------------+
                          |
                          v
                 +--------+--------+
                 |   Retriever     |
                 | (Topâ€‘K Search)  |
                 +--------+--------+
                          |
                          v
            +-------------+--------------+
            |          LLM Model         |
            |   (GPT, Claude, etc.)      |
            |  Combines query + context   |
            +-------------+--------------+
                          |
                          v
                 +--------+--------+
                 |  Final Response  |
                 | (Answer Output)  |
                 +------------------+
```

##  One-Page Summary**


ChromaDB = Fast vector database for storing embeddings + semantic search.
           Perfect for RAG and LLM apps.



# __________________________________________________________________________________
* # Break code on purpose â†’ fix & learn
# __________________________________________________________________________________


ollama run llama3




# **PART 1 â€” INGEST WORKFLOW**
          (You upload document â†’ create chunks â†’ embed â†’ store in Chroma)



{
  "path": "D:/PYTHON FULL STACK DEVELOPMENT/DJANGO_FLASK_CLASS/AI/rag_project1/data/notes.txt"
}
{
  "question": "What is Python decorator?"
}


| Layer                      | Purpose                                          |
| -------------------------- | ------------------------------------------------ |
| **1. Embeddings (Ollama)** | Convert text into meaningful numbers             |
| **2. ChromaDB**            | Store those vectors and retrieve similar chunks  |
| **3. Chunking**            | Break big documents into small, searchable parts |
| **4. RAG Pipeline**        | Query â†’ retrieve chunks â†’ generate answer        |
| **5. Flask API**           | Expose everything as HTTP endpoints              |





# _________________________________________________________

What Is Ollama? 
    Ollama is an offline platform that lets you run LLM models locally on your own system.
    It does not require internet, and no data goes to any cloud.

âœ” Runs fully offline
âœ” Supports many open-source LLMs
âœ” Works on Windows, macOS, Linux


# __________________________________________________
What Ollama Can Do
   âœ” Download open-source models

Like:
  Llama 3 / 3.1 / 3.2
  Qwen 2.5
  Phi-3
  Mistral / Mixtral
  DeepSeek-R1
  StarCoder2
  Gemma 2

# __________________________________________________

Why Companies Use Ollama

| Benefit                | Explanation                                 |
| ---------------------- | ------------------------------------------- |
| **Privacy**            | No data leaves your laptop or server        |
| **Cost saving**        | No API charges like GPT/Claude              |
| **Full control**       | You choose the model, version, quantization |
| **Offline capability** | Works without internet                      |
| **Fast inference**     | Uses GPU/CPU efficiently                    |


# __________________________________________________

Ollama Is NOT a Model â€” It Is a Platform

Ollama = local LLM engine
LLM = actual model (Llama, Qwen, etc.)


          Text documents
                 |
      [Embedding Model]
     (bge, e5, llama-embed)
                 â†“
         VECTOR embeddings
                 |
        Vector Database
                 |
         Query â†’ Embedding
                 |
         Similarity Search
                 |
        Top chunks retrieved
                 |
      [Chat Model - GPT / Llama]
                 â†“
            Final Answer


Install **Python 3.10+**
python --version
[https://ollama.com](https://ollama.com)



# __________________________________________________



ollama --version
ollama pull llama3
ollama pull nomic-embed-text

ollama serve
   Error: listen tcp 127.0.0.1:11434: bind:
   Only one usage of each socket address is normally permitted

   Ollama server already background-la run aagudhu,
   models install pannita,
   ippo direct ah Flask RAG app run panna podhum

ollama list
   


python app.py


# __________________________________________________


chroma.sqlite3 is being used by another process
   ğŸ‘‰ Your Flask app (Python) is still running
   ğŸ‘‰ ChromaDB keeps chroma.sqlite3 OPEN
   ğŸ‘‰ Windows does NOT allow deleting open files
    So PowerShell cannot delete db folder.

Windows-la file open irundha delete panna mudiyadhu
Flask + ChromaDB sqlite file use pannitu irukkum
CTRL+C / taskkill panna app stop aagi
apram Remove-Item work aagum



taskkill /IM python.exe /F  --use to  close powershell
Remove-Item db -Recurse -Force
python rag.py

DO NOT delete DB at app startup in production
You currently have code that resets Chroma every time.
That causes locks + crashes.

# __________________________________________________
FINAL TEST FLOW (Clean)

1ï¸âƒ£ CTRL + C
2ï¸âƒ£ Remove-Item db -Recurse -Force
3ï¸âƒ£ python rag.py
4ï¸âƒ£ /ingest
5ï¸âƒ£ /ask




# __________________________________________________
client = chromadb.PersistentClient(path="db")



Idhu ChromaDB client
ğŸ‘‰ path="db" kuduthurukkom na:

ğŸ”¹ Data ellam hard disk-la save aagum
ğŸ”¹ App stop pannalum data delete aagathu
ğŸ”¹ Next time app start pannalum data irukkum
PersistentClient na ChromaDB-la data disk-la permanent-ah store pannra client



# __________________________________________________
client.reset()
print("ğŸ”¥ Chroma reset successfully.")


ChromaDB-la already irukkura ellaa data / collections clear panna
WHY use pannrom?
Old vectors irukkum
Old embeddings mismatch aagum
Testing time-la confusion varum
â€œFresh-ah start pannaâ€



# __________________________________________________
shutil.rmtree(DB_PATH)
print("ğŸ”¥ Old DB folder deleted.")



Disk-la irukkura old Chroma files delete pannrom
WHY?

SQLite file corrupt aagirukkalam
Old index mismatch
Fresh DB create panna easy
â€œHard resetâ€





# __________________________________________________
except PermissionError:
    print("âŒ Windows locked the DB folder.")


Meaning:
Flask / Python still DB use pannitu irundha
Windows delete panna allow pannaadhu
â€œFile open irundha Windows lock pannumâ€

# __________________________________________________